{"cells":[{"cell_type":"markdown","source":["🤖 THE ABC 123 GROUP ™ 🤖\n","\n","🌐 GENERAL CONSULTING ABC 123 BY OSAROPRIME ™.\n","\n","🌐 ABC 123 USA ™\n","\n","🌐 ABC 123 DESYGN ™\n","\n","🌐 ABC 123 FILMS ™\n","\n","=============================================================\n","\n","         🌐 MAGENTRON ™ 🌐\n","🌐 ARTIFICIAL INTELLIGENCE 2.0 ™ : FOR MAKING IMAGINATION PROXIA G-2\n","\n","*️⃣📶🤖\n","\n","REQUIREMENTS:\n","\n","[*] Software Requirements: Python\n","\n","[*] HARDWARE REQUIREMENTS: fast GPU (Graphics Processing Unit)\n","\n","[*] DEPENDENCIES: \n","\n","- LIBRARIES: torch, transformers, diffusers, numpy, PIL, tqdm, difflib. \n","\n","- Weights and Biases account.\n","\n","\n","\n","This Google Colab NOTEBOOK will guide you on one possible scheme to create an IMAGINATION PROXIA in the ARTIFICIAL INTELLIGENCE 2.0™ FRAMEWORK/DOCUMENTATION. This NOTEBOOK will guide you one generating an IMAGE from text and then fine tuning/editing IMAGE through subsequent/additional text prompts (using Weights & Biases). You can adapt this to your needs. \n","\n","This PROXIA will endow the ROBOT with ability to EDIT/fine tune its IMAGINATION by making changes to the IMAGE via text.\n","\n","\n","\n","EXAMPLE USAGE:\n","\n","e.g On an ASTRAL MINDCLOUD this PROXIA can be used to process INFORMATION sent to it from an INSTINCTIVE MIND PROXIA/MINDCLOUD (OBJECT DETECTION). So for example if the ROBOT or a SWARM/HIVE/PHALANX of ROBOTS encounters an intersting object they can use their eye cameras to IMAGES of the subject which can be used as INPUT for this IMAGINATION PROXIA. This can help ROBOTS better understand the subject or environment of the IMAGE (as well as how humans view it). [SEE EXAMPLE IMAGES]\n","\n","e.g DREAMING: IMAGINATION PROXIAS CAN BE USED BY THE ROBOTS TO \"DREAM\". BY DREAMING I MEAN WHEN THE ROBOT IS IN HIBERNATION/SLEEP MODE IT CAN STILL PROCESS INFORMATION ABOUT THE OUTSIDE WORLD ON A LIMITED BASIS FROM PERIODIC TEXT PROMPTS (e.g from news, police reports).\n","\n","=============================================================\n","\n","CLICK ON THE FOLLOWING LINKS FOR JUPYTER NOTEBOOKS ON MAKING IMAGINATION PROXIA:\n","\n","https://github.com/GCABC123/magnetron.artificial-intelligence-2.0.mincloud.proxia--IMAGINATION-A1\n","\n","https://github.com/GCABC123/magnetron.artificial-intelligence-2.0.mincloud.proxia--IMAGINATION-B\n","\n","https://github.com/GCABC123/magnetron.artificial-intelligence-2.0.mincloud.proxia--IMAGINATION-C\n","\n","https://github.com/GCABC123/magnetron.artificial-intelligence-2.0.mincloud.proxia--IMAGINATION-D\n","\n","https://github.com/GCABC123/magnetron.artificial-intelligence-2.0.mincloud.proxia--IMAGINATION-G\n","\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","\n","Prerequisite reading:\n","\n","🌐 ARTIFICIAL INTELLIGENCE PRIMER ™: https://www.facebook.com/artificialintelligenceprimer\n","\n","🌐 ARTIFICIAL INTELLIGENCE 2.0 ™ DOCUMENTATION: https://www.facebook.com/aibyabc123/\n","\n","🌐 MEMBER'S CLUB ™ DOCUMENTATION - https://www.facebook.com/abc123membersclub/\n","\n","👑 INCLUDED STICKERS/SIGN:\n","\n","FIND STICKERS HERE: https://bit.ly/3B8D3lE\n","\n","PROMOTIONAL MATERIAL FOR 𝗠𝗔𝗚𝗡𝗘𝗧𝗥𝗢𝗡 𝗧𝗘𝗖𝗛𝗡𝗢𝗟𝗢𝗚𝗬 ™. (CUSTOM GRAPHICS BY 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗗𝗘𝗦𝗬𝗚𝗡 ™/𝗢𝗦𝗔𝗥𝗢 𝗛𝗔𝗥𝗥𝗜𝗢𝗧𝗧). THE 𝗠𝗔𝗚𝗡𝗘𝗧𝗥𝗢𝗡 𝗧𝗘𝗖𝗛𝗡𝗢𝗟𝗢𝗚𝗬 ™ SYMBOL/LOGO IS A TRADEMARK OF 𝗧𝗛𝗘 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗚𝗥𝗢𝗨𝗣 ™ FOR 𝗠𝗔𝗚𝗡𝗘𝗧𝗥𝗢𝗡 𝗧𝗘𝗖𝗛𝗡𝗢𝗟𝗢𝗚𝗬 ™. 𝗧𝗛𝗘 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗚𝗥𝗢𝗨𝗣 ™ SYMBOL/LOGO IS A TRADEMARK OF 𝗧𝗛𝗘 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗚𝗥𝗢𝗨𝗣 ™. *️⃣📶🤖\n","\n","PROMOTIONAL MATERIAL FOR 𝗔𝗥𝗧𝗜𝗙𝗜𝗖𝗜𝗔𝗟 𝗜𝗡𝗧𝗘𝗟𝗟𝗜𝗚𝗘𝗡𝗖𝗘 𝟮.𝟬 ™. (CUSTOM GRAPHICS BY 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗗𝗘𝗦𝗬𝗚𝗡 ™/𝗢𝗦𝗔𝗥𝗢 𝗛𝗔𝗥𝗥𝗜𝗢𝗧𝗧) THE 𝗗𝗥𝗔𝗚𝗢𝗡 & 𝗖𝗥𝗢𝗪𝗡 👑 SYMBOL/LOGO IS A TRADEMARK OF 𝗧𝗛𝗘 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗚𝗥𝗢𝗨𝗣 ™ ASSOCIATED WITH TECHNOLOGY. 𝗧𝗛𝗘 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗚𝗥𝗢𝗨𝗣 ™ SYMBOL/LOGO IS A TRADEMARK OF 𝗧𝗛𝗘 𝗔𝗕𝗖 𝟭𝟮𝟯 𝗚𝗥𝗢𝗨𝗣 ™. You must display the included stickers/signs (so that it is clearly visible) if you are working with MAGNETRON ™ TECHNOLOGY for the purposes of determining whether you want to purchase a technology license or not. This includes but is not limited to public technology displays, trade shows, technology expos, media appearances, Investor events, Computers (exterior), MINDCLOUD STORAGE (e.g server room doors, render farm room doors) etc.\n","\n",".\n","\n","🌐 NOTE: IMAGINATION PROXIA A IS DESCRIBED IN THE 𝗔𝗥𝗧𝗜𝗙𝗜𝗖𝗜𝗔𝗟 𝗜𝗡𝗧𝗘𝗟𝗟𝗜𝗚𝗘𝗡𝗖𝗘 𝟮.𝟬 ™ DOCUMENTATION.\n","\n","🌐 NOTE: 𝗔𝗥𝗧𝗜𝗙𝗜𝗖𝗜𝗔𝗟 𝗜𝗡𝗧𝗘𝗟𝗟𝗜𝗚𝗘𝗡𝗖𝗘 𝟮.𝟬 ™ is part of MAGNETRON ™ TECHNOLOGY.\n","\n","\n"],"metadata":{"id":"DqY5cjDfmSZc"}},{"cell_type":"markdown","source":["# 🔥🔥 Cross-Attention Control with Stable Diffusion + WandB Playground 🪄🐝\n","\n","<!--- @wandbcode{cross-attention-control-tmp} -->\n","\n","This is an implementation of Prompt-to-Prompt Image Editing\n","with Cross Attention Control using [Stable Diffusion](https://github.com/CompVis/stable-diffusion), [HuggingFace Diffusers](https://github.com/huggingface/diffusers) and [Weights & Biases](https://wandb.ai/site)."],"metadata":{"id":"bDLvF9KJf_mV"}},{"cell_type":"markdown","source":["# Step 1: Setup required libraries"],"metadata":{"id":"0J1IpII-kK0-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_rIUU9kd2cV","cellView":"form"},"outputs":[],"source":["#@title\n","\n","!pip install -q diffusers transformers ftfy wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ty2rvG3rePwe","cellView":"form"},"outputs":[],"source":["#@title\n","\n","import io\n","import wandb\n","import random\n","import numpy as np\n","from PIL import Image\n","from tqdm.auto import tqdm\n","from difflib import SequenceMatcher\n","from google.colab import files as colab_files\n","\n","import torch\n","from torch import autocast\n","\n","from transformers import CLIPModel, CLIPTextModel, CLIPTokenizer\n","from diffusers import (\n","    AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",")"]},{"cell_type":"markdown","source":["# Step 2: Set up Models and Weights & Biases Run\n","\n","- `wandb_project`: Weights & Biases project.\n","- `wandb_project`: Weights & Biases entity.\n","- `huggingface_access_token`: HuggingFace Access Token. Check out this page from the official HuggingFace docs as to how to generate your own access token.\n","- `config.device`: Accelerator device. Choose `mps` if you're running the code on an M1 Mac.\n","- `config.model_path_clip`: Alias for pre-trained CLIP Model.\n","- `config.model_path_diffusion`: Alias for pre-trained Stable Diffusion Model."],"metadata":{"id":"icR8uhL9kUI2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mk2Mogzwecpo","cellView":"form"},"outputs":[],"source":["wandb_project = \"cross-attention-control\" #@param {\"type\": \"string\"}\n","wandb_entity = \"wandb\" #@param {\"type\": \"string\"}\n","\n","wandb.init(project=wandb_project, entity=wandb_entity, job_type=\"generate\")\n","config = wandb.config\n","\n","huggingface_access_token = \"\" #@param {\"type\": \"string\"}\n","torch_dtype = torch.float16\n","\n","config.model_precision_type = \"fp16\"\n","config.device = \"cuda\" #@param['cuda', 'cpu', 'mps']\n","config.model_path_clip = \"openai/clip-vit-large-patch14\" #@param['openai/clip-vit-large-patch14']\n","config.model_path_diffusion = \"CompVis/stable-diffusion-v1-4\" #@param['CompVis/stable-diffusion-v1-4']\n","\n","\n","clip_tokenizer = CLIPTokenizer.from_pretrained(config.model_path_clip)\n","clip_model = CLIPModel.from_pretrained(\n","    config.model_path_clip,\n","    torch_dtype=torch_dtype\n",")\n","clip = clip_model.text_model\n","\n","\n","model_path_diffusion = \"CompVis/stable-diffusion-v1-4\"\n","unet = UNet2DConditionModel.from_pretrained(\n","    model_path_diffusion,\n","    subfolder=\"unet\",\n","    use_auth_token=huggingface_access_token,\n","    revision=config.model_precision_type,\n","    torch_dtype=torch.float16\n",")\n","vae = AutoencoderKL.from_pretrained(\n","    model_path_diffusion,\n","    subfolder=\"vae\",\n","    use_auth_token=huggingface_access_token,\n","    revision=config.model_precision_type,\n","    torch_dtype=torch.float16\n",")\n","\n","\n","unet.to(config.device)\n","vae.to(config.device)\n","clip.to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iZXdCVPfLDc","cellView":"form"},"outputs":[],"source":["#@title\n","\n","def init_attention_weights(weight_tuples):\n","    tokens_length = clip_tokenizer.model_max_length\n","    weights = torch.ones(tokens_length)\n","    \n","    for i, w in weight_tuples:\n","        if i < tokens_length and i >= 0:\n","            weights[i] = w\n","    \n","    \n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn2\" in name:\n","            module.last_attn_slice_weights = weights.to(config.device)\n","        if module_name == \"CrossAttention\" and \"attn1\" in name:\n","            module.last_attn_slice_weights = None\n","    \n","\n","def init_attention_edit(tokens, tokens_edit):\n","    tokens_length = clip_tokenizer.model_max_length\n","    mask = torch.zeros(tokens_length)\n","    indices_target = torch.arange(tokens_length, dtype=torch.long)\n","    indices = torch.zeros(tokens_length, dtype=torch.long)\n","\n","    tokens = tokens.input_ids.numpy()[0]\n","    tokens_edit = tokens_edit.input_ids.numpy()[0]\n","    \n","    for name, a0, a1, b0, b1 in SequenceMatcher(\n","        None, tokens, tokens_edit\n","    ).get_opcodes():\n","        if b0 < tokens_length:\n","            if name == \"equal\" or (name == \"replace\" and a1-a0 == b1-b0):\n","                mask[b0:b1] = 1\n","                indices[b0:b1] = indices_target[a0:a1]\n","\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn2\" in name:\n","            module.last_attn_slice_mask = mask.to(config.device)\n","            module.last_attn_slice_indices = indices.to(config.device)\n","        if module_name == \"CrossAttention\" and \"attn1\" in name:\n","            module.last_attn_slice_mask = None\n","            module.last_attn_slice_indices = None\n","\n","\n","def init_attention_func():\n","    def new_attention(self, query, key, value, sequence_length, dim):\n","        batch_size_attention = query.shape[0]\n","        hidden_states = torch.zeros(\n","            (batch_size_attention, sequence_length, dim // self.heads), device=query.device, dtype=query.dtype\n","        )\n","        slice_size = self._slice_size if self._slice_size is not None else hidden_states.shape[0]\n","        for i in range(hidden_states.shape[0] // slice_size):\n","            start_idx = i * slice_size\n","            end_idx = (i + 1) * slice_size\n","            attn_slice = (\n","                torch.einsum(\"b i d, b j d -> b i j\", query[start_idx:end_idx], key[start_idx:end_idx]) * self.scale\n","            )\n","            attn_slice = attn_slice.softmax(dim=-1)\n","            \n","            if self.use_last_attn_slice:\n","                if self.last_attn_slice_mask is not None:\n","                    new_attn_slice = torch.index_select(self.last_attn_slice, -1, self.last_attn_slice_indices)\n","                    attn_slice = attn_slice * (1 - self.last_attn_slice_mask) + new_attn_slice * self.last_attn_slice_mask\n","                else:\n","                    attn_slice = self.last_attn_slice\n","                \n","                self.use_last_attn_slice = False\n","                    \n","            if self.save_last_attn_slice:\n","                self.last_attn_slice = attn_slice\n","                self.save_last_attn_slice = False\n","                \n","            if self.use_last_attn_weights and self.last_attn_slice_weights is not None:\n","                attn_slice = attn_slice * self.last_attn_slice_weights\n","                self.use_last_attn_weights = False\n","\n","            attn_slice = torch.einsum(\"b i j, b j d -> b i d\", attn_slice, value[start_idx:end_idx])\n","\n","            hidden_states[start_idx:end_idx] = attn_slice\n","\n","        # reshape hidden_states\n","        hidden_states = self.reshape_batch_dim_to_heads(hidden_states)\n","        return hidden_states\n","\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\":\n","            module.last_attn_slice = None\n","            module.use_last_attn_slice = False\n","            module.use_last_attn_weights = False\n","            module.save_last_attn_slice = False\n","            module._attention = new_attention.__get__(module, type(module))\n","            \n","def use_last_tokens_attention(use=True):\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn2\" in name:\n","            module.use_last_attn_slice = use\n","            \n","def use_last_tokens_attention_weights(use=True):\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn2\" in name:\n","            module.use_last_attn_weights = use\n","            \n","def use_last_self_attention(use=True):\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn1\" in name:\n","            module.use_last_attn_slice = use\n","            \n","def save_last_tokens_attention(save=True):\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn2\" in name:\n","            module.save_last_attn_slice = save\n","            \n","def save_last_self_attention(save=True):\n","    for name, module in unet.named_modules():\n","        module_name = type(module).__name__\n","        if module_name == \"CrossAttention\" and \"attn1\" in name:\n","            module.save_last_attn_slice = save\n","\n","\n","def postprocess(image):\n","    image = (image / 2 + 0.5).clamp(0, 1)\n","    image = image.cpu().permute(0, 2, 3, 1).numpy()\n","    image = (image[0] * 255).round().astype(\"uint8\")\n","    return Image.fromarray(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_TrNFOhqQxx","cellView":"form"},"outputs":[],"source":["#@title\n","\n","@torch.no_grad()\n","def stablediffusion(\n","    prompt=\"\",\n","    prompt_edit=\"\",\n","    prompt_edit_token_weights=[],\n","    prompt_edit_tokens_start=0.0,\n","    prompt_edit_tokens_end=1.0,\n","    prompt_edit_spatial_start=0.0,\n","    prompt_edit_spatial_end=1.0,\n","    guidance_scale=7.5,\n","    steps=50,\n","    seed=None,\n","    width=512,\n","    height=512,\n","    init_image=None,\n","    init_image_strength=0.5,\n","):\n","    log_key = (\n","        \"Generated Image without Promp Edit\"\n","        if prompt_edit == \"\"\n","        else \"Generated Image with Promp Edit\"\n","    )\n","    print(log_key)\n","\n","    # Change size to multiple of 64 to prevent size mismatches inside model\n","    width = width - width % 64\n","    height = height - height % 64\n","    \n","    #If seed is None, randomly select seed from 0 to 2^32-1\n","    if seed is None: seed = random.randrange(2**32 - 1)\n","    generator = torch.cuda.manual_seed(seed)\n","    \n","    # Set inference timesteps to scheduler\n","    scheduler = LMSDiscreteScheduler(\n","        beta_start=0.00085,\n","        beta_end=0.012,\n","        beta_schedule=\"scaled_linear\",\n","        num_train_timesteps=1000\n","    )\n","    scheduler.set_timesteps(steps)\n","    \n","    # Preprocess image if it exists (img2img)\n","    if init_image is not None:\n","        #Resize and transpose for numpy b h w c -> torch b c h w\n","        init_image = init_image.resize(\n","            (width, height), resample=Image.LANCZOS\n","        )\n","        init_image = np.array(\n","            init_image\n","        ).astype(np.float32) / 255.0 * 2.0 - 1.0\n","        init_image = torch.from_numpy(\n","            init_image[np.newaxis, ...].transpose(0, 3, 1, 2)\n","        )\n","        \n","        # If there is alpha channel, composite alpha for white,\n","        # as the diffusion model does not support alpha channel\n","        if init_image.shape[1] > 3:\n","            init_image = init_image[:, :3] * init_image[:, 3:] + (\n","                1 - init_image[:, 3:]\n","            )\n","            \n","        #Move image to GPU\n","        init_image = init_image.to(config.device)\n","        \n","        #Encode image\n","        with autocast(config.device):\n","            init_latent = vae.encode(\n","                init_image\n","            ).latent_dist.sample(generator=generator) * 0.18215\n","            \n","        t_start = steps - int(steps * init_image_strength)\n","            \n","    else:\n","        init_latent = torch.zeros(\n","            (1, unet.in_channels, height // 8, width // 8),\n","            device=config.device\n","        )\n","        t_start = 0\n","    \n","    # Generate random normal noise\n","    noise = torch.randn(\n","        init_latent.shape, generator=generator, device=config.device\n","    )\n","    latent = scheduler.add_noise(\n","        init_latent, noise, t_start\n","    ).to(config.device)\n","    \n","    # Process clip\n","    with autocast(config.device):\n","        tokens_unconditional = clip_tokenizer(\n","            \"\",\n","            padding=\"max_length\",\n","            max_length=clip_tokenizer.model_max_length,\n","            truncation=True,\n","            return_tensors=\"pt\",\n","            return_overflowing_tokens=True\n","        )\n","        embedding_unconditional = clip(\n","            tokens_unconditional.input_ids.to(config.device)\n","        ).last_hidden_state\n","\n","        tokens_conditional = clip_tokenizer(\n","            prompt,\n","            padding=\"max_length\",\n","            max_length=clip_tokenizer.model_max_length,\n","            truncation=True,\n","            return_tensors=\"pt\",\n","            return_overflowing_tokens=True\n","        )\n","        embedding_conditional = clip(\n","            tokens_conditional.input_ids.to(config.device)\n","        ).last_hidden_state\n","\n","        # Process prompt editing\n","        if prompt_edit != \"\":\n","            tokens_conditional_edit = clip_tokenizer(\n","                prompt_edit,\n","                padding=\"max_length\",\n","                max_length=clip_tokenizer.model_max_length,\n","                truncation=True,\n","                return_tensors=\"pt\",\n","                return_overflowing_tokens=True\n","            )\n","            embedding_conditional_edit = clip(\n","                tokens_conditional_edit.input_ids.to(config.device)\n","            ).last_hidden_state\n","            \n","            init_attention_edit(\n","                tokens_conditional, tokens_conditional_edit\n","            )\n","            \n","        init_attention_func()\n","        init_attention_weights(prompt_edit_token_weights)\n","            \n","        timesteps = scheduler.timesteps[t_start:]\n","        \n","        for i, t in tqdm(enumerate(timesteps), total=len(timesteps)):\n","            t_index = t_start + i\n","\n","            sigma = scheduler.sigmas[t_index]\n","            latent_model_input = latent\n","            latent_model_input = (\n","                latent_model_input / ((sigma**2 + 1) ** 0.5)\n","            ).to(unet.dtype)\n","\n","            # Predict the unconditional noise residual\n","            noise_pred_uncond = unet(\n","                latent_model_input,\n","                t,\n","                encoder_hidden_states=embedding_unconditional\n","            ).sample\n","            \n","            # Prepare the Cross-Attention layers\n","            if prompt_edit is not None:\n","                save_last_tokens_attention()\n","                save_last_self_attention()\n","            else:\n","                #Use weights on non-edited prompt when edit is None\n","                use_last_tokens_attention_weights()\n","                \n","            # Predict the conditional noise residual and save\n","            # the cross-attention layer activations\n","            noise_pred_cond = unet(\n","                latent_model_input,\n","                t,\n","                encoder_hidden_states=embedding_conditional\n","            ).sample\n","            \n","            # Edit the Cross-Attention layer activations\n","            if prompt_edit != \"\":\n","                t_scale = t / scheduler.num_train_timesteps\n","                if t_scale >= prompt_edit_tokens_start and t_scale <= prompt_edit_tokens_end:\n","                    use_last_tokens_attention()\n","                if t_scale >= prompt_edit_spatial_start and t_scale <= prompt_edit_spatial_end:\n","                    use_last_self_attention()\n","                    \n","                # Use weights on edited prompt\n","                use_last_tokens_attention_weights()\n","\n","                # Predict the edited conditional noise residual\n","                # using the cross-attention masks\n","                noise_pred_cond = unet(\n","                    latent_model_input,\n","                    t,\n","                    encoder_hidden_states=embedding_conditional_edit\n","                ).sample\n","                \n","            #Perform guidance\n","            noise_pred = noise_pred_uncond + guidance_scale * (\n","                noise_pred_cond - noise_pred_uncond\n","            )\n","\n","            latent = scheduler.step(noise_pred, t_index, latent).prev_sample\n","\n","            wandb.log({\n","                log_key: wandb.Image(\n","                    postprocess(\n","                        vae.decode((latent / 0.18215).to(vae.dtype)).sample\n","                    )\n","                )\n","            }, step=i)\n","\n","        # scale and decode the image latents with vae\n","        latent = latent / 0.18215\n","        image = vae.decode(latent.to(vae.dtype)).sample\n","\n","    return postprocess(image)"]},{"cell_type":"markdown","source":["# Step 3: Enter Prompts and Additional Configs\n","\n","- `config.prompt`: The prompt as a string.\n","- `config.prompt_edit`: The second prompt as a string, used to edit the first prompt using cross attention, set `\"\"` to disable.\n","- `config.prompt_edit_token_weights`: Values to scale the importance of the tokens in cross attention layers, as a list of tuples representing `(token id, strength)`, this is used to increase or decrease the importance of a word in the prompt, it is applied to prompt_edit when possible (if `prompt_edit` is `\"\"`, weights are applied to prompt).\n","- `config.prompt_edit_tokens_start`: How strict is the generation with respect to the initial prompt, increasing this will let the network be more creative for smaller details/textures, should be smaller than `prompt_edit_tokens_end`.\n","- `config.prompt_edit_tokens_end`: How strict is the generation with respect to the initial prompt, decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than `prompt_edit_tokens_start`.\n","- `config.prompt_edit_spatial_start`: How strict is the generation with respect to the initial image (generated from the first prompt, not from img2img), increasing this will let the network be more creative for smaller details/textures, should be smaller than `prompt_edit_spatial_end`.\n","- `config.prompt_edit_spatial_end`: How strict is the generation with respect to the initial image (generated from the first prompt, not from img2img), decreasing this will let the network be more creative for larger features/general scene composition, should be bigger than `prompt_edit_spatial_start`.\n","- `config.guidance_scale`: Standard classifier-free guidance strength for stable diffusion.\n","- `config.steps`: Number of diffusion steps as an integer, higher usually produces better images but is slower.\n","- `config.seed`: Random Seed.\n","- `config.image_width`: Width of generated image.\n","- `config.image_height`: Height of generated image."],"metadata":{"id":"rh2De-3dl3-9"}},{"cell_type":"code","source":["def display_prompt_tokens(prompt):\n","    tokens = clip_tokenizer(\n","        prompt,\n","        padding=\"max_length\",\n","        max_length=clip_tokenizer.model_max_length,\n","        truncation=True,\n","        return_tensors=\"pt\",\n","        return_overflowing_tokens=True\n","    ).input_ids[0]\n","    for idx, token in enumerate(tokens):\n","        decoded_token = clip_tokenizer.decode(token)\n","        if decoded_token == \"<|startoftext|>\":\n","            continue\n","        elif decoded_token == \"<|endoftext|>\":\n","            break\n","        else:\n","            print(idx, \"->\", decoded_token)\n","\n","\n","# the prompt as a string\n","config.prompt = \"A photo of a Person with flower headpiece and elegant jewels\" #@param {\"type\": \"string\"}\n","\n","# the second prompt as a string, used to edit the first prompt\n","# using cross attention, set \"\" to disable\n","config.prompt_edit = \"A photo of a Person with butterfly headpiece and elegant jewels\" #@param {\"type\": \"string\"}\n","\n","display_prompt_tokens(config.prompt_edit)"],"metadata":{"cellView":"form","id":"jGr3m8p7sqfn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgAJbmfivgjc","cellView":"form"},"outputs":[],"source":["# values to scale the importance of the tokens in\n","# cross attention layers, as a list of tuples representing\n","# (token id, strength), this is used to increase or decrease\n","# the importance of a word in the prompt, it is applied to prompt_edit when possible (if prompt_edit is None, weights are applied to prompt)\n","config.prompt_edit_token_weights = [(7, 4)] #@param {type:\"raw\"}\n","\n","# how strict is the generation with respect to the initial prompt,\n","# increasing this will let the network be more creative for smaller\n","# details/textures, should be smaller than prompt_edit_tokens_end\n","config.prompt_edit_tokens_start = 0.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","# how strict is the generation with respect to the initial prompt,\n","# decreasing this will let the network be more creative for larger\n","# features/general scene composition, should be bigger than\n","# prompt_edit_tokens_start\n","config.prompt_edit_tokens_end = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","# how strict is the generation with respect to the initial image\n","# (generated from the first prompt, not from img2img), increasing\n","# this will let the network be more creative for smaller\n","# details/textures, should be smaller than prompt_edit_spatial_end\n","config.prompt_edit_spatial_start = 0.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","# how strict is the generation with respect to the initial image\n","# (generated from the first prompt, not from img2img), decreasing\n","# this will let the network be more creative for larger\n","# features/general scene composition, should be bigger than\n","# prompt_edit_spatial_start\n","config.prompt_edit_spatial_end = 0.8 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","\n","# standard classifier-free guidance strength for stable diffusion\n","config.guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:100, step:0.1}\n","\n","# number of diffusion steps as an integer, higher usually produces\n","# better images but is slower\n","config.steps = 50 #@param {type:\"slider\", min:0, max:1000, step:1}\n","\n","# random seed as an integer\n","config.seed = 98374234 #@param {type:\"number\"}\n","\n","# image width and heigh\n","config.image_width = 768 #@param {type:\"slider\", min:512, max:1024, step:1}\n","config.image_height = 512 #@param {type:\"slider\", min:512, max:1024, step:1}"]},{"cell_type":"markdown","source":["# Step 4: Generate Images with Prompt and Prompt Edits.\n","\n","Image genetaed will be automatically logged to the respective **Weights & Biases** workspace as an interactive [**Table**](https://docs.wandb.ai/guides/data-vis) with all configs.\n","\n","![](https://i.imgur.com/CqIJgPg.png)"],"metadata":{"id":"3mE4ZhjEn2tU"}},{"cell_type":"code","source":["#@title\n","\n","generated_image_with_prompt_edit = stablediffusion(\n","    prompt=config.prompt,\n","    prompt_edit=config.prompt_edit,\n","    prompt_edit_token_weights=config.prompt_edit_token_weights,\n","    prompt_edit_tokens_start=config.prompt_edit_tokens_start,\n","    prompt_edit_tokens_end=config.prompt_edit_tokens_end,\n","    prompt_edit_spatial_start=config.prompt_edit_spatial_start,\n","    prompt_edit_spatial_end=config.prompt_edit_spatial_end,\n","    guidance_scale=config.guidance_scale,\n","    steps=config.steps,\n","    seed=config.seed,\n","    width=config.image_width,\n","    height=config.image_height,\n","    init_image=None,\n","    init_image_strength=0.5\n",")\n","\n","if config.prompt_edit != \"\":\n","    generated_image_without_prompt_edit = stablediffusion(\n","        prompt=config.prompt,\n","        prompt_edit=\"\",\n","        prompt_edit_token_weights=config.prompt_edit_token_weights,\n","        prompt_edit_tokens_start=config.prompt_edit_tokens_start,\n","        prompt_edit_tokens_end=config.prompt_edit_tokens_end,\n","        prompt_edit_spatial_start=config.prompt_edit_spatial_start,\n","        prompt_edit_spatial_end=config.prompt_edit_spatial_end,\n","        guidance_scale=config.guidance_scale ,\n","        steps=config.steps,\n","        seed=config.seed,\n","        width=config.image_width,\n","        height=config.image_height,\n","        init_image=None,\n","        init_image_strength=0.5\n","    )\n","    table = wandb.Table(\n","        columns=[\n","            \"Seed\",\n","            \"Guidance Scale\",\n","            \"Image Height\",\n","            \"Image Width\",\n","            \"Number of Steps\",\n","            \"Prompt\",\n","            \"Image Generated With Prompt\",\n","            \"Prompt Edit\",\n","            \"Edit Token Weights\",\n","            \"Image Generated With Prompt Edit\"\n","        ]\n","    )\n","    table.add_data(\n","        config.seed,\n","        config.guidance_scale,\n","        config.image_height,\n","        config.image_width,\n","        config.steps,\n","        config.prompt,\n","        wandb.Image(generated_image_without_prompt_edit),\n","        config.prompt_edit,\n","        config.prompt_edit_token_weights,\n","        wandb.Image(generated_image_with_prompt_edit)\n","    )\n","    wandb.log({\n","        \"Image Editing with Cross Attention Control\": table\n","    })\n","\n","\n","wandb.finish()"],"metadata":{"id":"ltVIFaB50gf0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**References:**\n","- https://arxiv.org/abs/2208.01626\n","- https://github.com/bloc97/CrossAttentionControl"],"metadata":{"id":"_kr1FmAhjL76"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[{"file_id":"1wAMyOp4SZyVZU6Pc-wAS5QSJS_0g7a4C","timestamp":1665081869874}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}